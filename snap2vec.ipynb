{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# snap2vec\n",
    "\n",
    "usinn stanford social network datasets + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gmerz/person2vec/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"ee4b0178-ca54-4022-b002-d28d3c641e7b\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#ee4b0178-ca54-4022-b002-d28d3c641e7b\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"ee4b0178-ca54-4022-b002-d28d3c641e7b\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'ee4b0178-ca54-4022-b002-d28d3c641e7b' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#ee4b0178-ca54-4022-b002-d28d3c641e7b\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#ee4b0178-ca54-4022-b002-d28d3c641e7b\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you'll probably get a lot of warnings here -- sorry \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import hdbscan\n",
    "% matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to write both sparsified and regular forms of our graph to adjaceny list. We want to test word2vec on various sparsified graphs and see if we can predict links that occur in the full on adjacency list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write Full graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing adjacency list\n",
      "Writing to file\n"
     ]
    }
   ],
   "source": [
    "! python preprocess_snaps.py twitter_combined.txt 1 directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write 90% of edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing adjacency list\n",
      "Writing to file\n"
     ]
    }
   ],
   "source": [
    "! python preprocess_snaps.py twitter_combined.txt .9 directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write 80% of edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing adjacency list\n",
      "Writing to file\n"
     ]
    }
   ],
   "source": [
    "! python preprocess_snaps.py twitter_combined.txt .8 directed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write 70% of edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing adjacency list\n",
      "Writing to file\n"
     ]
    }
   ],
   "source": [
    "! python preprocess_snaps.py twitter_combined.txt .7 directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adjListFromFile(fpath):\n",
    "    ''' \n",
    "        function that constructs a dictionary \n",
    "        of nodes -> list of nodes their connected to\n",
    "        (an adjacency list representation of the graph)\n",
    "        from a file\n",
    "    '''\n",
    "    adjList = {}\n",
    "    with open(fpath, 'r') as f: \n",
    "        for i, line in enumerate(f): \n",
    "            if i != 0: \n",
    "                totalList = line.strip().split(' ')\n",
    "                adjList[totalList[0]] = [x.strip() for x in totalList[1:]]\n",
    "    return adjList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullGraph = adjListFromFile('twitter_combine1_lst.txt')\n",
    "partialGraph = adjListFromFile('twitter_combine0.9_lst.txt')\n",
    "partialGraphS = adjListFromFile('twitter_combine0.7_lst.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70097 67904 69505\n"
     ]
    }
   ],
   "source": [
    "# sanity check \n",
    "print(len(fullGraph), len(partialGraphS), len(partialGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testNodes(partialGraph, fullGraph):\n",
    "    '''\n",
    "        another quick sanity check: function that \n",
    "        ensures every node in the partial graph\n",
    "        is also in the full graph \n",
    "        \n",
    "        (this wasn't the case before -- there was a bug\n",
    "        where spaces weren't written but i fixed hehe)\n",
    "    '''\n",
    "    num_errors = 0 \n",
    "    for node_id in partialGraph.keys(): \n",
    "        if node_id not in fullGraph.keys(): \n",
    "            num_errors += 1 \n",
    "    return num_errors\n",
    "\n",
    "assert not(testNodes(partialGraph, fullGraph))\n",
    "assert not(testNodes(partialGraphS, fullGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING GRAPH\n",
      "WRITING TO FILE\n"
     ]
    }
   ],
   "source": [
    "! python documentize.py twitter_combine0.9_lst.txt twitter.9vec.txt .01 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING GRAPH\n",
      "WRITING TO FILE\n"
     ]
    }
   ],
   "source": [
    "! python documentize.py twitter_combine0.7_lst.txt twitter.7vec.txt .01 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 183 ms, total: 15.1 s\n",
      "Wall time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(LineSentence('twitter.7vec.txt'), size=100, window=5, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testPrediction(fullGraph, partialGraph, model):\n",
    "    numExisting, numFound, numWrong = 0, 0, 0\n",
    "    for i, user_id in enumerate(model.vocab.keys()):\n",
    "        if i % 1000 == 0: \n",
    "            print(\"On word %s out of %s\" % (i, len(model.vocab)))\n",
    "        most_similar = [t[0] for t in model.most_similar(user_id)[:5]]\n",
    "        for similar_user in most_similar: \n",
    "            if similar_user in partialGraph[user_id]:\n",
    "                numExisting += 1\n",
    "            elif similar_user in fullGraph[user_id]: \n",
    "                numFound += 1 \n",
    "            else: \n",
    "                numWrong += 1 \n",
    "    return numExisting, numFound, numWrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On word 0 out of 16698\n",
      "On word 1000 out of 16698\n",
      "On word 2000 out of 16698\n",
      "On word 3000 out of 16698\n",
      "On word 4000 out of 16698\n",
      "On word 5000 out of 16698\n",
      "On word 6000 out of 16698\n",
      "On word 7000 out of 16698\n",
      "On word 8000 out of 16698\n",
      "On word 9000 out of 16698\n",
      "On word 10000 out of 16698\n",
      "On word 11000 out of 16698\n",
      "On word 12000 out of 16698\n",
      "On word 13000 out of 16698\n",
      "On word 14000 out of 16698\n",
      "On word 15000 out of 16698\n",
      "On word 16000 out of 16698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18264, 3752, 61474)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPrediction(fullGraph, partialGraphS, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison with LinkPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import linkpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy of the file just renamed .edgelist so it knows what to do lmaooo\n",
    "G = linkpred.read_network(\"twitter_combined.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simrank = linkpred.predictors.SimRank(G, excluded=G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
